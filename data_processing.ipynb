{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/ITDP_PrestigeLogo.png)\n",
    "\n",
    "## Limpieza Inicial de Datos, Unión de Tablas y Formateo de Fechas\n",
    "\n",
    "Se requiere de un conjunto de datos limpio, es decir, que no se presenten entradas nulas o NaN’s, que el formato de fechas sea el mismo para todos los valores y que los atributos estén en forma de columnas i.e. que cada variable meteorológica o de contaminantes estén en una columna separada, entre otras propiedades que se describirán a continuación.\n",
    "\n",
    "El proceso de limpieza de datos consiste en hacer un conjunto de manipulaciones a la tablas para generar un dataset óptimo. A continuación, se muestra el diagrama de la limpieza de datos realizada:\n",
    "\n",
    "\n",
    "<img src=\"./assets/diagrama.png\" style=\"height:200px\">\n",
    "\n",
    "__Pasos y descripción general del notebook__\n",
    "\n",
    "1. __Descarga de Tablas:__ Los datos de contaminantes y meteorología son descargados por separado. Los datos usados para el entrenamiento son verificados de manera manual por la SEDEMA. En este notebook vamos a juntar los archivos de contaminación y meoteorología de cada año en un solo archivo, también se eliminan las entradas vacías. \n",
    "\n",
    "\n",
    "2. __Convertir a tabla con variables por columna__: Se pasa de tener una columna que indica el atributo medido y otro el valor de la medición a una columna por cada atribute que indica el valor de la medición.\n",
    "\n",
    "\n",
    "3. __Formateo de Fechas:__ se arreglará el formato de fechas al formato **YY/m/d hh:mm** con horas de 0 a 23 y también vamos a generar columnas de información temporal con parámetros como hora, día y mes para cada medición \n",
    "\n",
    "\n",
    "- __Datos recibidos:__ [Meteorología,](http://www.aire.cdmx.gob.mx/default.php?opc='aKBhnmI='&opcion=Zw==)\n",
    "[Contamianción](http://www.aire.cdmx.gob.mx/default.php?opc='aKBhnmI='&opcion=Zg==)\n",
    "- __Responsable:__ Daniel Bustillos\n",
    "- __Contacto:__  juandaniel.bucam@gmail.com\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from datetime import timedelta\n",
    "import datetime as dt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentación de los datos utilizados\n",
    "\n",
    "El Sistema de Monitoreo Atmosférico de la Ciudad de México presenta de forma horaria desde el año 1986 las condiciones meteorológicas y de contaminación que describen la atmósfera de la zona metropolitana. La información descrita se presenta de dos formas: puede ser una base de datos revisada por expertos de la SEDEMA para descartar mediciones de fuentes atípicas de contaminación tales como incendios o desperfectos en las estaciones de monitoreo, o no revisada, obteniendo directamente la medición como se midió en la estación de monitoreo. Esta falta de consistencia de la información puede generar valores erróneos en el pronóstico generado, limitando el desempeño de los modelos. Por este motivo, los datos de monitoreo usados para el entrenamiento de los modelos son los datos revisados por los expertos.\n",
    "\n",
    "Para el entrenamiento de los modelos los datos usados abarcan el periodo de enero 2014 hasta diciembre 2018, accesibles en la sección de datos Horarios por contaminante y de datos horarios de Meteorología. Las variables meteorológicas y de contaminación utilizadas para el desarrollo del modelo se muestra en la siguiente tabla:\n",
    "\n",
    "<img src=\"./assets/table_attributes.png\" style=\"height:250px\">\n",
    "\n",
    "Las estaciones en operación se distribuyen en el área metropolitana, concentrándose en la zona central de la CDMX. En la siguiente figura se muestra la posición geográﬁca de las estaciones.\n",
    "\n",
    "<img src=\"./assets/mapa.png\" style=\"height:350px\">\n",
    "\n",
    "Como parte del proceso de la generación de los modelos de pronóstico de contamianción, es necesario realizar un conjunto de operaciones a los datos obtenidos de la página de [Monitoreo de Calidad del Aire de la Ciudad de México](http://www.aire.cdmx.gob.mx/default.php). Como se mencionó en el archivo de metodología, los datos a usar son los datos verificados por los expertos de la SEDEMA. Los datos para meteorología y contaminanción se pueden obtener acontinuación:\n",
    "\n",
    "- [Meteorología](http://www.aire.cdmx.gob.mx/default.php?opc='aKBhnmI='&opcion=Zw==)\n",
    "- [Contamianción](http://www.aire.cdmx.gob.mx/default.php?opc='aKBhnmI='&opcion=Zg==)\n",
    "\n",
    "\n",
    "\n",
    "Juntaremos los dataframes con una PivotTable y las agruparemos por el momento de la medición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definimos tres funciones para formatear el formato de las fechas:\n",
    "\n",
    "Convertir el formato de 1 a 24 horas al formato de 0 a 23 horas. Por defecto python trabaja con el formato de 0 a 23 horas, es conveniente trabajar en este formato debido a que muchas de las funciones implementadas en python u otras librerias suponen que este es el formato de las fechas.\n",
    "\n",
    "El formato original de las fechas, es d/m/YY h:m y el formato despuésde aplicar la función es YY/m/d hh:mm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_converter(x):\n",
    "    x0 = x.split(\" \")[0]\n",
    "    x0 = x0.split(\"/\")\n",
    "    x1 = x.split(\" \")[1]\n",
    "    if x1[:].endswith(\"24:00\"):\n",
    "        # Notemos que cuando la hora es 24, es necesario convertirla a 00 sin embargo también es necesario\n",
    "        # esta fecha se desplazará al siguiente día, es deicr, si se tiene '19-05-01 24:00', al terminar con \"24\",\n",
    "        # se sustituirá por '19-05-02 00:00'\n",
    "        # Considerando esto, se aplica lasiguiente función:\n",
    "        fecha_0 = x0[2]+\"-\"+x0[1]+\"-\"+x0[0]+\" 23:00\"\n",
    "        date = datetime.strptime(fecha_0, \"%Y-%m-%d %H:%M\")\n",
    "        new_time = date + timedelta(hours=1)\n",
    "        return new_time.strftime('%Y-%m-%d %H:%M')\n",
    "    else:\n",
    "        return x0[2]+\"-\"+x0[1]+\"-\"+x0[0]+\" \"+ x1[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos el año a limpiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"meteorologia\"\n",
    "target = \"contaminantes\"\n",
    "anio = \"2015\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se define una función que realizará los siguientes procesos:\n",
    " \n",
    " - Leer el archivo de contaminantes o meteorología del año seleccionado.\n",
    " - Eliminar las entradas vacías\n",
    " - Hacer una tabla pivote para pasar de una columna con el nombre del atributo y su valor a una columna por atributo.\n",
    " - Convertir la columna fecha de d/m/yy hh:mm a yy/mm/dd hh:mm y pasar del formato de horas de 1..24 a 0...23.\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "met_2018 = pd.read_csv(str('./datasets/' + target + \"/\" + target + \"_\" + str(anio) + \".csv\"),header=10) # leer archivo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if \"cve_station\" in met_2018.columns or \"cve_parameter\" in met_2018.columns:\n",
    "    met_2018.rename(columns={'cve_station': 'id_station', 'cve_parameter': 'id_parameter'}, inplace=True) # checar nombre columbas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "met_2018['hora'] = met_2018['date'].astype(str).str[-5:-3].astype(int)\n",
    "met_2018 = met_2018.dropna(subset=[\"value\"]).reset_index(drop=True)#PM25"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.distplot(met_2018[\"hora\"], bins=24, kde=False, rug=True);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for hora in tqdm(range(1,25)): # valores por estación\n",
    "    estaciones.loc[:,hora] = met_2018[met_2018[\"hora\"]==hora][\"id_station\"].value_counts().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juntemos este proceso en una función, se aplicará a meteorología y contaminantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formateo_csv(target, anio):   \n",
    "    #leemos el archivo\n",
    "    met_2018 = pd.read_csv(str('../data/raw/' + target + \"/\" + target + \"_\" + str(anio) + \".csv\"),header=10)\n",
    "    if \"cve_station\" in met_2018.columns or \"cve_parameter\" in met_2018.columns:\n",
    "        met_2018.rename(columns={'cve_station': 'id_station', 'cve_parameter': 'id_parameter'}, inplace=True)\n",
    "    #eliminamos las entradas vacías\n",
    "    met_2018 = met_2018.dropna(how='any')\n",
    "    met_2018 = met_2018.drop(['unit'], axis=1)\n",
    "    \n",
    "    met_ACO = met_2018\n",
    "    met_ACO = met_ACO.reset_index(drop=False)\n",
    "    met_ACO = met_ACO[[\"date\",\"id_station\",\"id_parameter\",\"value\"]] # nos quedamos con las siguientes columnas:\n",
    "\n",
    "    #Hacer una tabla pivote para pasar de una columna con el nombre del atributo\n",
    "                            # y su valor a una columna por atributo.\n",
    "    met_ACO_hour = pd.pivot_table(met_ACO,index=[\"date\",\"id_station\"],columns=[\"id_parameter\"])\n",
    "    met_ACO_hour = met_ACO_hour.reset_index(drop=False)\n",
    "    met_ACO_hour.columns = met_ACO_hour.columns.droplevel()\n",
    "    met_ACO_hour[\"id_station\"] = met_ACO_hour.iloc[:,1]\n",
    "    met_ACO_hour[\"date\"] = met_ACO_hour.iloc[:,0]\n",
    "    \n",
    "    #eliminamos la columna vacía\n",
    "    met_ACO_hour = met_ACO_hour.drop([\"\"],axis=1)\n",
    "    \n",
    "    # Convertir la columna fecha de d/m/yy hh:mm a yy/mm/dd hh:mm y pasar del formato de horas de 1..24 a 0...23.\n",
    "    met_ACO_hour['date'] = met_ACO_hour.apply(lambda row: time_converter(row['date']), axis=1) \n",
    "    met_ACO_hour['date'] =  pd.to_datetime(met_ACO_hour['date'], format='%Y-%m-%d %H:%M')\n",
    "    met_ACO_hour = met_ACO_hour.rename(columns={'date': 'fecha'})\n",
    "    \n",
    "    return(met_ACO_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos la función anterior para los datos de metereología y contaminantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../data/raw/meteorologia/meteorologia_2019.csv does not exist: '../data/raw/meteorologia/meteorologia_2019.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3a31506c59ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"meteorologia\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0manio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2019\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmeteorologia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformateo_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtarget2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"contaminantes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcontaminacion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformateo_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-89a39ec50523>\u001b[0m in \u001b[0;36mformateo_csv\u001b[0;34m(target, anio)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mformateo_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#leemos el archivo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmet_2018\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/raw/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manio\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"cve_station\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmet_2018\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"cve_parameter\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmet_2018\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmet_2018\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'cve_station'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'id_station'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cve_parameter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'id_parameter'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../data/raw/meteorologia/meteorologia_2019.csv does not exist: '../data/raw/meteorologia/meteorologia_2019.csv'"
     ]
    }
   ],
   "source": [
    "target1 = \"meteorologia\"\n",
    "anio = \"2019\"\n",
    "meteorologia = formateo_csv(target1, anio)\n",
    "target2 = \"contaminantes\"\n",
    "contaminacion = formateo_csv(target2, anio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorologia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge de Dataframes   <a class=\"anchor\" id=\"merge-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juntamos los dataframes generados, así podremos trabajar con ambos archivos a la vez:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hour_merge = pd.merge(meteorologia, contaminacion, on=[\"fecha\",\"id_station\"],how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos 3 columnas con la información temporal del momento en que se tomó la medición\n",
    "en la columna de fecha se elimina la información de hora y minuto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hour_merge['hora'] = data_hour_merge['fecha'].astype(str).str[10:13].astype(int)\n",
    "data_hour_merge['dia'] = data_hour_merge['fecha'].astype(str).str[8:10].astype(int)\n",
    "data_hour_merge['mes'] = data_hour_merge['fecha'].astype(str).str[5:7].astype(int)\n",
    "#  data_hour_merge['fecha'] = data_hour_merge['fecha'].astype(str).str[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hour_merge.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Una vez que corroboramos el correcto funcionamiento del proceso, podemos juntar los pasos anteriores en una función y así agilizar el proceso de la limpieza de cada año:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_parser(anio_1):\n",
    "    print(anio_1)\n",
    "    target1 = \"meteorologia\"\n",
    "    meteorologia = formateo_csv(target1, anio_1)\n",
    "    target2 = \"contaminantes\"\n",
    "    contaminacion = formateo_csv(target2, anio_1)\n",
    "\n",
    "    data_hour_merge = pd.merge(meteorologia, contaminacion, on=[\"fecha\",\"id_station\"],how=\"outer\")\n",
    "\n",
    "    data_hour_merge['hora'] = data_hour_merge['fecha'].astype(str).str[10:13]\n",
    "    data_hour_merge['dia'] = data_hour_merge['fecha'].astype(str).str[8:10]\n",
    "    data_hour_merge['mes'] = data_hour_merge['fecha'].astype(str).str[5:7]\n",
    "    #  data_hour_merge['fecha'] = data_hour_merge['fecha'].astype(str).str[0:10]\n",
    "    data_hour_merge.to_csv(str(\"../data/processed/met_cont_hora/cont_hora\"+ \n",
    "                               str(anio_1) +\".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corremos la función desde el 2012 al 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a3b5e83c532f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mdata_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0manio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2015\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2021\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-a3b5e83c532f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mdata_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0manio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2015\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2021\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_parser' is not defined"
     ]
    }
   ],
   "source": [
    "[data_parser(str(anio)) for anio in range(2015,2021)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
