{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/ITDP_PrestigeLogo.png)\n",
    "\n",
    "## Limpieza Inicial de Datos, Unión de Tablas y Formateo de Fechas\n",
    "\n",
    "Se requiere de un conjunto de datos limpio, es decir, que no se presenten entradas nulas o NaN’s, que el formato de fechas sea el mismo para todos los valores y que los atributos estén en forma de columnas i.e. que cada variable meteorológica o de contaminantes estén en una columna separada, entre otras propiedades que se describirán a continuación.\n",
    "\n",
    "El proceso de limpieza de datos consiste en hacer un conjunto de manipulaciones a la tablas para generar un dataset óptimo. A continuación, se muestra el diagrama de la limpieza de datos realizada:\n",
    "\n",
    "\n",
    "<img src=\"./assets/diagrama.png\" style=\"height:200px\">\n",
    "\n",
    "__Pasos y descripción general del notebook__\n",
    "\n",
    "1. __Descarga de Tablas:__ Los datos de contaminantes y meteorología son descargados por separado. Los datos usados para el entrenamiento son verificados de manera manual por la SEDEMA. En este notebook vamos a juntar los archivos de contaminación y meoteorología de cada año en un solo archivo, también se eliminan las entradas vacías. \n",
    "\n",
    "\n",
    "2. __Convertir a tabla con variables por columna__: Se pasa de tener una columna que indica el atributo medido y otro el valor de la medición a una columna por cada atribute que indica el valor de la medición.\n",
    "\n",
    "\n",
    "3. __Formateo de Fechas:__ se arreglará el formato de fechas al formato **YY/m/d hh:mm** con horas de 0 a 23 y también vamos a generar columnas de información temporal con parámetros como hora, día y mes para cada medición \n",
    "\n",
    "\n",
    "- __Datos recibidos:__ [Meteorología,](http://www.aire.cdmx.gob.mx/default.php?opc='aKBhnmI='&opcion=Zw==)\n",
    "[Contamianción](http://www.aire.cdmx.gob.mx/default.php?opc='aKBhnmI='&opcion=Zg==)\n",
    "- __Responsable:__ Daniel Bustillos\n",
    "- __Contacto:__  juandaniel.bucam@gmail.com\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from datetime import timedelta\n",
    "import datetime as dt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentación de los datos utilizados\n",
    "\n",
    "El Sistema de Monitoreo Atmosférico de la Ciudad de México presenta de forma horaria desde el año 1986 las condiciones meteorológicas y de contaminación que describen la atmósfera de la zona metropolitana. La información descrita se presenta de dos formas: puede ser una base de datos revisada por expertos de la SEDEMA para descartar mediciones de fuentes atípicas de contaminación tales como incendios o desperfectos en las estaciones de monitoreo, o no revisada, obteniendo directamente la medición como se midió en la estación de monitoreo. Esta falta de consistencia de la información puede generar valores erróneos en el pronóstico generado, limitando el desempeño de los modelos. Por este motivo, los datos de monitoreo usados para el entrenamiento de los modelos son los datos revisados por los expertos.\n",
    "\n",
    "Para el entrenamiento de los modelos los datos usados abarcan el periodo de enero 2014 hasta diciembre 2018, accesibles en la sección de datos Horarios por contaminante y de datos horarios de Meteorología. Las variables meteorológicas y de contaminación utilizadas para el desarrollo del modelo se muestra en la siguiente tabla:\n",
    "\n",
    "<img src=\"./assets/table_attributes.png\" style=\"height:250px\">\n",
    "\n",
    "Las estaciones en operación se distribuyen en el área metropolitana, concentrándose en la zona central de la CDMX. En la siguiente figura se muestra la posición geográﬁca de las estaciones.\n",
    "\n",
    "<img src=\"./assets/mapa.png\" style=\"height:350px\">\n",
    "\n",
    "Como parte del proceso de la generación de los modelos de pronóstico de contamianción, es necesario realizar un conjunto de operaciones a los datos obtenidos de la página de [Monitoreo de Calidad del Aire de la Ciudad de México](http://www.aire.cdmx.gob.mx/default.php). Como se mencionó en el archivo de metodología, los datos a usar son los datos verificados por los expertos de la SEDEMA. Los datos para meteorología y contaminanción se pueden obtener acontinuación:\n",
    "\n",
    "- [Meteorología](http://www.aire.cdmx.gob.mx/default.php?opc='aKBhnmI='&opcion=Zw==)\n",
    "- [Contamianción](http://www.aire.cdmx.gob.mx/default.php?opc='aKBhnmI='&opcion=Zg==)\n",
    "\n",
    "\n",
    "\n",
    "Juntaremos los dataframes con una PivotTable y las agruparemos por el momento de la medición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definimos tres funciones para formatear el formato de las fechas:\n",
    "\n",
    "Convertir el formato de 1 a 24 horas al formato de 0 a 23 horas. Por defecto python trabaja con el formato de 0 a 23 horas, es conveniente trabajar en este formato debido a que muchas de las funciones implementadas en python u otras librerias suponen que este es el formato de las fechas.\n",
    "\n",
    "El formato original de las fechas, es d/m/YY h:m y el formato despuésde aplicar la función es YY/m/d hh:mm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_converter(x):\n",
    "    x0 = x.split(\" \")[0]\n",
    "    x0 = x0.split(\"/\")\n",
    "    x1 = x.split(\" \")[1]\n",
    "    if x1[:].endswith(\"24:00\"):\n",
    "        # Notemos que cuando la hora es 24, es necesario convertirla a 00 sin embargo también es necesario\n",
    "        # esta fecha se desplazará al siguiente día, es deicr, si se tiene '19-05-01 24:00', al terminar con \"24\",\n",
    "        # se sustituirá por '19-05-02 00:00'\n",
    "        # Considerando esto, se aplica lasiguiente función:\n",
    "        fecha_0 = x0[2]+\"-\"+x0[1]+\"-\"+x0[0]+\" 23:00\"\n",
    "        date = datetime.strptime(fecha_0, \"%Y-%m-%d %H:%M\")\n",
    "        new_time = date + timedelta(hours=1)\n",
    "        return new_time.strftime('%Y-%m-%d %H:%M')\n",
    "    else:\n",
    "        return x0[2]+\"-\"+x0[1]+\"-\"+x0[0]+\" \"+ x1[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos el año a limpiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"meteorologia\"\n",
    "target = \"contaminantes\"\n",
    "anio = \"2015\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se define una función que realizará los siguientes procesos:\n",
    " \n",
    " - Leer el archivo de contaminantes o meteorología del año seleccionado.\n",
    " - Eliminar las entradas vacías\n",
    " - Hacer una tabla pivote para pasar de una columna con el nombre del atributo y su valor a una columna por atributo.\n",
    " - Convertir la columna fecha de d/m/yy hh:mm a yy/mm/dd hh:mm y pasar del formato de horas de 1..24 a 0...23.\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "met_2018 = pd.read_csv(str('./datasets/' + target + \"/\" + target + \"_\" + str(anio) + \".csv\"),header=10) # leer archivo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if \"cve_station\" in met_2018.columns or \"cve_parameter\" in met_2018.columns:\n",
    "    met_2018.rename(columns={'cve_station': 'id_station', 'cve_parameter': 'id_parameter'}, inplace=True) # checar nombre columbas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "met_2018['hora'] = met_2018['date'].astype(str).str[-5:-3].astype(int)\n",
    "met_2018 = met_2018.dropna(subset=[\"value\"]).reset_index(drop=True)#PM25"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.distplot(met_2018[\"hora\"], bins=24, kde=False, rug=True);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for hora in tqdm(range(1,25)): # valores por estación\n",
    "    estaciones.loc[:,hora] = met_2018[met_2018[\"hora\"]==hora][\"id_station\"].value_counts().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juntemos este proceso en una función, se aplicará a meteorología y contaminantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formateo_csv(target, anio):   \n",
    "    #leemos el archivo\n",
    "    met_2018 = pd.read_csv(str('./data/raw/' + target + \"/\" + target + \"_\" + str(anio) + \".csv\"),header=10)\n",
    "    if \"cve_station\" in met_2018.columns or \"cve_parameter\" in met_2018.columns:\n",
    "        met_2018.rename(columns={'cve_station': 'id_station', 'cve_parameter': 'id_parameter'}, inplace=True)\n",
    "    #eliminamos las entradas vacías\n",
    "    met_2018 = met_2018.dropna(how='any')\n",
    "    met_2018 = met_2018.drop(['unit'], axis=1)\n",
    "    \n",
    "    met_ACO = met_2018\n",
    "    met_ACO = met_ACO.reset_index(drop=False)\n",
    "    met_ACO = met_ACO[[\"date\",\"id_station\",\"id_parameter\",\"value\"]] # nos quedamos con las siguientes columnas:\n",
    "\n",
    "    #Hacer una tabla pivote para pasar de una columna con el nombre del atributo\n",
    "                            # y su valor a una columna por atributo.\n",
    "    met_ACO_hour = pd.pivot_table(met_ACO,index=[\"date\",\"id_station\"],columns=[\"id_parameter\"])\n",
    "    met_ACO_hour = met_ACO_hour.reset_index(drop=False)\n",
    "    met_ACO_hour.columns = met_ACO_hour.columns.droplevel()\n",
    "    met_ACO_hour[\"id_station\"] = met_ACO_hour.iloc[:,1]\n",
    "    met_ACO_hour[\"date\"] = met_ACO_hour.iloc[:,0]\n",
    "    \n",
    "    #eliminamos la columna vacía\n",
    "    met_ACO_hour = met_ACO_hour.drop([\"\"],axis=1)\n",
    "    \n",
    "    # Convertir la columna fecha de d/m/yy hh:mm a yy/mm/dd hh:mm y pasar del formato de horas de 1..24 a 0...23.\n",
    "    met_ACO_hour['date'] = met_ACO_hour.apply(lambda row: time_converter(row['date']), axis=1) \n",
    "    met_ACO_hour['date'] =  pd.to_datetime(met_ACO_hour['date'], format='%Y-%m-%d %H:%M')\n",
    "    met_ACO_hour = met_ACO_hour.rename(columns={'date': 'fecha'})\n",
    "    \n",
    "    return(met_ACO_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos la función anterior para los datos de metereología y contaminantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1 = \"meteorologia\"\n",
    "anio = \"2019\"\n",
    "meteorologia = formateo_csv(target1, anio)\n",
    "target2 = \"contaminantes\"\n",
    "contaminacion = formateo_csv(target2, anio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id_parameter</th>\n",
       "      <th>RH</th>\n",
       "      <th>TMP</th>\n",
       "      <th>WDR</th>\n",
       "      <th>WSP</th>\n",
       "      <th>id_station</th>\n",
       "      <th>fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>ACO</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>191.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>AJM</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>AJU</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>BJU</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>CHO</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "id_parameter    RH   TMP    WDR  WSP id_station               fecha\n",
       "0             74.0  10.0  318.0  0.9        ACO 2019-01-01 01:00:00\n",
       "1             53.0   NaN  191.0  5.1        AJM 2019-01-01 01:00:00\n",
       "2              NaN   NaN  197.0  2.3        AJU 2019-01-01 01:00:00\n",
       "3             65.0  12.7  154.0  1.4        BJU 2019-01-01 01:00:00\n",
       "4             63.0   7.3  100.0  1.1        CHO 2019-01-01 01:00:00"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteorologia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge de Dataframes   <a class=\"anchor\" id=\"merge-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juntamos los dataframes generados, así podremos trabajar con ambos archivos a la vez:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hour_merge = pd.merge(meteorologia, contaminacion, on=[\"fecha\",\"id_station\"],how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos 3 columnas con la información temporal del momento en que se tomó la medición\n",
    "en la columna de fecha se elimina la información de hora y minuto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hour_merge['hora'] = data_hour_merge['fecha'].astype(str).str[10:13].astype(int)\n",
    "data_hour_merge['dia'] = data_hour_merge['fecha'].astype(str).str[8:10].astype(int)\n",
    "data_hour_merge['mes'] = data_hour_merge['fecha'].astype(str).str[5:7].astype(int)\n",
    "#  data_hour_merge['fecha'] = data_hour_merge['fecha'].astype(str).str[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id_parameter</th>\n",
       "      <th>RH</th>\n",
       "      <th>TMP</th>\n",
       "      <th>WDR</th>\n",
       "      <th>WSP</th>\n",
       "      <th>id_station</th>\n",
       "      <th>fecha</th>\n",
       "      <th>CO</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOX</th>\n",
       "      <th>O3</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PMCO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>hora</th>\n",
       "      <th>dia</th>\n",
       "      <th>mes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>ACO</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>191.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>AJM</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>AJU</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>BJU</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>CHO</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "id_parameter    RH   TMP    WDR  WSP id_station               fecha  CO  NO  \\\n",
       "0             74.0  10.0  318.0  0.9        ACO 2019-01-01 01:00:00 NaN NaN   \n",
       "1             53.0   NaN  191.0  5.1        AJM 2019-01-01 01:00:00 NaN NaN   \n",
       "2              NaN   NaN  197.0  2.3        AJU 2019-01-01 01:00:00 NaN NaN   \n",
       "3             65.0  12.7  154.0  1.4        BJU 2019-01-01 01:00:00 NaN NaN   \n",
       "4             63.0   7.3  100.0  1.1        CHO 2019-01-01 01:00:00 NaN NaN   \n",
       "\n",
       "id_parameter  NO2  NOX  O3   PM10  PM2.5  PMCO  SO2  hora  dia  mes  \n",
       "0             NaN  NaN NaN  139.0    NaN   NaN  NaN     1    1    1  \n",
       "1             NaN  NaN NaN   42.0   23.0  19.0  NaN     1    1    1  \n",
       "2             NaN  NaN NaN    NaN    NaN   NaN  NaN     1    1    1  \n",
       "3             NaN  NaN NaN   64.0   47.0  17.0  NaN     1    1    1  \n",
       "4             NaN  NaN NaN   81.0    NaN   NaN  NaN     1    1    1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hour_merge.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Una vez que corroboramos el correcto funcionamiento del proceso, podemos juntar los pasos anteriores en una función y así agilizar el proceso de la limpieza de cada año:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_parser(anio_1):\n",
    "    print(anio_1)\n",
    "    target1 = \"meteorologia\"\n",
    "    meteorologia = formateo_csv(target1, anio_1)\n",
    "    target2 = \"contaminantes\"\n",
    "    contaminacion = formateo_csv(target2, anio_1)\n",
    "\n",
    "    data_hour_merge = pd.merge(meteorologia, contaminacion, on=[\"fecha\",\"id_station\"],how=\"outer\")\n",
    "\n",
    "    data_hour_merge['hora'] = data_hour_merge['fecha'].astype(str).str[10:13]\n",
    "    data_hour_merge['dia'] = data_hour_merge['fecha'].astype(str).str[8:10]\n",
    "    data_hour_merge['mes'] = data_hour_merge['fecha'].astype(str).str[5:7]\n",
    "    #  data_hour_merge['fecha'] = data_hour_merge['fecha'].astype(str).str[0:10]\n",
    "    data_hour_merge.to_csv(str(\"../data/processed/met_cont_hora/cont_hora\"+ \n",
    "                               str(anio_1) +\".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corremos la función desde el 2012 al 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/processed/met_cont_hora/cont_hora2015.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a3b5e83c532f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mdata_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0manio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2015\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2021\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-a3b5e83c532f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mdata_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0manio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2015\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2021\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-911bfe0a7c53>\u001b[0m in \u001b[0;36mdata_parser\u001b[0;34m(anio_1)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#  data_hour_merge['fecha'] = data_hour_merge['fecha'].astype(str).str[0:10]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     data_hour_merge.to_csv(str(\"../data/processed/met_cont_hora/cont_hora\"+ \n\u001b[0;32m---> 15\u001b[0;31m                                str(anio_1) +\".csv\"), index=False)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/processed/met_cont_hora/cont_hora2015.csv'"
     ]
    }
   ],
   "source": [
    "[data_parser(str(anio)) for anio in range(2015,2021)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
